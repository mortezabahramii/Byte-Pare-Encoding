{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "\n",
    "def get_vocab(filename):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_tokens(vocab):\n",
    "    tokens = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "    return tokens\n",
    "\n",
    "vocab = get_vocab('TestText.txt')\n",
    "print(vocab)\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')\n",
    "\n",
    "num_merges = 500\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iteration = 1000\n",
    "\n",
    "import re, collections\n",
    "\n",
    "\n",
    "def get_vocab(filename):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_tokens(vocab):\n",
    "    tokens = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "    return tokens\n",
    "\n",
    "vocab = get_vocab('TestText.txt')\n",
    "print(vocab)\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')\n",
    "\n",
    "num_merges = 1000\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    \n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
